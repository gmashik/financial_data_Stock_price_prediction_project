# -*- coding: utf-8 -*-
"""Financial_data_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nFMIIRn_9PE67-lj15Z8-em6TbLaPSBi

# **Financial Data Project**

We'll focus on bank stocks and see how they progressed throughout the [financial crisis](https://en.wikipedia.org/wiki/Financial_crisis_of_2007%E2%80%9308) all the way to early 2016.
"""

#!pip install git+https://github.com/pydata/pandas-datareader.git

#please use this to download all the module which will be used here 
!wget https://raw.githubusercontent.com/gmashik/financial_data_project/master/my_utils.py

# Commented out IPython magic to ensure Python compatibility.
from pandas_datareader import data, wb
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
import datetime
from datetime import date
import pandas as pd
import numpy as np
from plotly import __version__
import plotly.offline as pyo
import plotly.graph_objs as go
from plotly.offline import iplot
from my_utils import *
import cufflinks as cf
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot 
# %matplotlib inline

"""# Download and load the data

In this section we will use pandas to directly read data from Google finance !
First we need to start with the proper imports.We need to get data using pandas datareader. We will get stock information for the following banks:
*  Bank of America
* CitiGroup
* Goldman Sachs
* JPMorgan Chase
* Morgan Stanley
* Wells Fargo
We've used [this documentation page](https://pandas-datareader.readthedocs.io/en/latest/remote_data.html) data loading instructions.
"""

from datetime import datetime
start = datetime(2000, 1, 1)
end = datetime(2020, 7, 31)

# Bank of America
BAC = data.DataReader("BAC", 'yahoo', start, end)
# CitiGroup
C = data.DataReader("C", 'yahoo', start, end)
# Goldman Sachs
GS = data.DataReader("GS", 'yahoo', start, end)
# JPMorgan Chase
JPM = data.DataReader("JPM", 'yahoo', start, end)
# Morgan Stanley
MS = data.DataReader("MS", 'yahoo', start, end)
# Apple
APPL = data.DataReader("AAPL", 'yahoo', start, end)

BAC.info()

keys = ['BAC', 'C', 'GS', 'JPM', 'MS', 'APPLE']
bank_stocks = pd.concat([BAC, C, GS, JPM, MS, APPL],axis=1,keys=keys)

bank_stocks

bank_stocks.columns.names = ['Institute Name','Stock Info']

bank_stocks

"""# **Exploratory Data Anlysis**

**What is the max Close price for each bank's stock throughout the time period?**
"""

bank_stocks.xs(key='Close',axis=1,level='Stock Info').max()

returns=pd.DataFrame()

for name in keys:
  returns[name+" Return"]=bank_stocks[name]['Close'].pct_change()

returns.tail()

sns.pairplot(returns[1:])

returns.idxmin()

"""**On the above dates each institue stock had the worst single day returns. We can see that 3 of the banks share the same day for the worst drop, did anything significant happen that day?**

Yes, the [financial crisis](https://en.wikipedia.org/wiki/Financial_crisis_of_2007%E2%80%9308) all the way to early 2016 probably responsible for that.
"""

returns.idxmax()

"""**On the above dates each bank stock had the best single day returns.**

We have also noticed that Citigroup's largest drop and biggest gain were very close to one another, did anythign significant happen in that time frame?

**probably  [Citigroup had a stock split.](https://www.google.com/webhp?sourceid=chrome-instant&ion=1&espv=2&ie=UTF-8#q=citigroup+stock+2011+may)**

** Take a look at the standard deviation of the returns, which stock would we can classify as the riskiest over the entire time period?**
"""

returns.std()

"""**we can classify Citigroup as the riskiest over the entire time period.**

Same analysis for 2020.
"""

returns.loc['2020-01-01':'2020-07-31'].std()

"""** Distribution of the 2020 returns for Morgan Stanley **"""

sns.distplot(returns.loc['2020-01-01':'2020-07-31']['MS Return'],color='green',bins=70)

"""Distribution of the 2020 returns for Citigroup"""

sns.distplot(returns.loc['2020-01-01':'2020-07-31']['C Return'],color='Blue',bins=100)

"""Distribution of the 2020 returns for Bank of America"""

sns.distplot(returns.loc['2020-01-01':'2020-07-31']['BAC Return'],color='Purple',bins=100)

sns.distplot(returns.loc['2020-01-01':'2020-07-31']['APPLE Return'],color='Green',bins=100)

for name in keys:
    bank_stocks[name]['Close'].plot(figsize=(12,4),label=name)
plt.legend()

"""##  line plot showing Close price for each bank for the entire index of time"""

configure_plotly_browser_state()
bank_stocks.xs(key='Close',axis=1,level='Stock Info').iplot()

"""### **We can noticed a drastical decline for all the institue at early 200. That probale reason is [Covid-19.](https://en.wikipedia.org/wiki/Coronavirus_disease_2019)**

## Moving Averages

Let's analyze the moving averages for these stocks in the year 2019 and 2020. 

** The rolling 30 day average against the Close Price for Bank Of America's stock for the year 2019 and 2020**
"""

plt.figure(figsize=(14,6))
BAC['Close'].loc['2019-01-01':'2020-07-31'].rolling(window=30).mean().plot(label='30 Day Avg')
BAC['Close'].loc['2019-01-01':'2020-07-31'].plot(label='BAC CLOSE')
plt.legend()

"""The rolling 7 day average against the Close Price for Bank Of America's stock for the year 2019 and 2020"""

plt.figure(figsize=(12,4))
BAC['Close'].loc['2019-01-01':'2020-07-31'].rolling(window=7).mean().plot(label="Weekkly Moving Average")
BAC['Close'].loc['2019-01-01':'2020-07-31'].plot(label="BAC close")
plt.legend()

"""**Heatmap of the correlation between the stocks Close Price**"""

sns.heatmap(bank_stocks.xs(key='Close',axis=1,level='Stock Info').corr(),annot=True)

"""Interactive plot"""

cuf=bank_stocks.xs(key='Close',axis=1,level='Stock Info').corr()
configure_plotly_browser_state()
cuf.iplot(kind='heatmap',colorscale='rdylbu')

sns.clustermap(bank_stocks.xs(key='Close',axis=1,level='Stock Info').corr(),annot=True)

"""**candle plot of Bank of America's stock from Jan 1st 2019 to August 31st 2020.**"""

configure_plotly_browser_state()
BAC[['Open', 'High', 'Low', 'Close']].loc['2019-01-01':'2020-07-31'].iplot(kind='candle')

"""**Simple Moving Averages plot of Morgan Stanley for the year 2019 and 2020.**"""

configure_plotly_browser_state()
MS['Close'].loc['2019-01-01':'2020-07-31'].ta_plot(study='sma',periods=[14,30,60],title='Simple Moving Averages')

"""**Bollinger Band Plot for Bank of America for the year 2019 and 2020.**"""

configure_plotly_browser_state()
BAC['Close'].loc['2019-01-01':'2020-07-31'].ta_plot(study='boll')

"""**candle plot of Apple's stock from Jan 1st 2019 to August 31st 2020.**"""

configure_plotly_browser_state()
APPL[['Open', 'High', 'Low', 'Close']].loc['2019-01-01':'2020-07-31'].iplot(kind='candle')

"""**Simple Moving Averages plot of Apple for the year 2019 and 2020.**"""

configure_plotly_browser_state()
APPL['Close'].loc['2019-01-01':'2020-07-31'].ta_plot(study='sma',periods=[14,30,60],title='Simple Moving Averages')

"""# **Build stock price prdiction model using LSTM for APPLE Inc.**"""

data=APPL.drop(['High', 'Low', 'Open', 'Volume', 'Adj Close'], axis=1)
data.to_csv('data.csv',sep=',')

data.head()

import csv
time_step = []
price = []
temp=0
with open('data.csv') as csvfile:
  reader = csv.reader(csvfile, delimiter=',')
  next(reader)
  for row in reader:
    price.append(float(row[1]))
    time_step.append(temp)
    temp+=1

series = np.array(price)
time = np.array(time_step)
plt.figure(figsize=(18, 7))
plot_series(time, series)

#We are going to take 10% data as our validation data
test_ind =  len(data)-int(len(data)*10/100)
time_train = time[:test_ind]
x_train = series[:test_ind]
time_valid = time[test_ind:]
x_valid = series[test_ind:]
window_size = 18
batch_size = 32
shuffle_buffer_size = 1000

tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
train_set = windowed_dataset(x_train, window_size=18, batch_size=100, shuffle_buffer=shuffle_buffer_size)
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=32, kernel_size=5,
                      strides=1, padding="causal",
                      activation="relu",
                      input_shape=[None, 1]),
  tf.keras.layers.LSTM(100, return_sequences=True),
  tf.keras.layers.LSTM(100, return_sequences=True),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 200)
])


optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set,epochs=100)

apple_forecast = os_model_forecast(model, series[..., np.newaxis], window_size)
apple_forecast = apple_forecast[test_ind - window_size:-1, -1, 0]
plt.figure(figsize=(16, 6))
plot_series(time_valid, x_valid,label='train_data')
plot_series(time_valid, apple_forecast,label='predicted_data')

plt.figure(figsize=(16, 6))
plot_series(time,series,label='original')
plot_series(time_valid, apple_forecast,label='forcast')

"""**This prediction is reasonable since the earlier prediction the price prediction was good. However the later prediction was not very good since we can see a very sharp decrease and very sharp increase.**"""

